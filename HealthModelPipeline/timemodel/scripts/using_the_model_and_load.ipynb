{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c718a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "model_upgrading_path = os.path.join(\"..\",\"src\")\n",
    "sys.path.append(model_upgrading_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3d9fd0a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'my_package.data.select_dataset_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# moduel\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmy_package\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtime_series\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmy_package\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselect_ship_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_ship_dataframe_from_database\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmy_package\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_database\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_database\n",
      "File \u001b[1;32m~\\Desktop\\dx_project\\techross\\health_learning_data\\time_model\\scripts\\..\\src\\my_package\\time_series.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# coding: utf-8\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# moduel\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmy_package\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselect_dataset_all\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_all_dataframe_from_database\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# basic\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'my_package.data.select_dataset_all'"
     ]
    }
   ],
   "source": [
    "# moduel\n",
    "import my_package.time_series as time\n",
    "from my_package.data.select_ship_dataset import get_ship_dataframe_from_database\n",
    "from my_package.data.load_database import load_database\n",
    "from my_package.data.select_dataset_all import get_all_dataframe_from_database\n",
    "from my_package.data.delete_from_database import delete_from_database\n",
    "\n",
    "# basic \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# learning\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609557b",
   "metadata": {},
   "source": [
    "클래스\n",
    "\n",
    "1. input: 선박 이름, output: 현재 op를 기반으로 다음 op의 전극 효율 평균 예측 )\n",
    "2. ConfiguringTimeData + TimeSeriesModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4bb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfiguringTimeData:\n",
    "   # 인풋 데이터 구축\n",
    "    def __init__(self, ship_name):\n",
    "        # 선박 이름을 이용한 선박 아이디 추출 \n",
    "        ship_id = ship_id = time.find_ship(ship_name)[0]\n",
    "        \n",
    "        # 데이터 로드 (과거 데이터)\n",
    "        self.ship_data = get_ship_dataframe_from_database('tc_ai_current_system_health', ship_id)\n",
    "        \n",
    "        # 해당 선박 데이터 생성\n",
    "        self.create_electrode_data()\n",
    "        \n",
    "        # 해당 선박 정제\n",
    "        self.preprocess_data()\n",
    "        \n",
    "        # 해당 선박 전극 효율 이동평균 생성\n",
    "        self.moving_average_df = time.generate_moving_average(self.electrod_df, 250)\n",
    "        \n",
    "        # 해당 선박 시간 그룹화\n",
    "        self.group_time_series()\n",
    "        \n",
    "        # 해당 테스트 데이터 생성\n",
    "        #self.test()\n",
    "        \n",
    "        # 부족한 시간 계산 및 학습 데이터 생성\n",
    "        #self.calculate_the_time()\n",
    "        \n",
    "    def create_electrode_data(self):\n",
    "        \"\"\" 데이터 로드 및 전처리\n",
    "        \"\"\"\n",
    "        # 1 .전처리\n",
    "        data  = self.ship_data[self.ship_data['ELECTRODE_EFFICIENCY'].notna()]\n",
    "        data = data[(data['ELECTRODE_EFFICIENCY']>-100)]\n",
    "        data = data.drop_duplicates()\n",
    "        \n",
    "        # 2. 결측치 제거\n",
    "        self.data = data.dropna()\n",
    "\n",
    "    \n",
    "    def group_time_series(self):\n",
    "        \"\"\"\n",
    "         시간 기준 데이터 그룹화( 효율 평균 )\n",
    "        \"\"\"\n",
    "    \n",
    "        # 1. 시간을 기준으로 전극 효율 계산\n",
    "        self.historic_data = self.moving_average_df[['DATA_TIME','Moving_Average']].groupby('DATA_TIME').mean().reset_index()\n",
    "\n",
    "        # 2. datetime 형식으로 변환\n",
    "        self.historic_data['DATA_TIME'] = pd.to_datetime(self.historic_data['DATA_TIME'])\n",
    "    \n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\" 데이터 정제 및 처리\n",
    "        \"\"\"\n",
    "        # 1. 순서 정렬\n",
    "        electrod_df = self.data.sort_values(by=['DATA_TIME'])\n",
    "        \n",
    "        # 2.시간을 년-월-일-시-분으로 추출\n",
    "        electrod_df['DATA_TIME'] = electrod_df['DATA_TIME'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "        # 3. 인덱스 재 설정\n",
    "        electrod_df = electrod_df.reset_index(drop=True)\n",
    "\n",
    "        # 4. 중복 값 제거\n",
    "        self.electrod_df = electrod_df.drop_duplicates()\n",
    "    \n",
    "    \n",
    "    def calculate_the_time(self):\n",
    "        \"\"\"\n",
    "            부족한 시간 추출\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. 학습을 위한 요구 시간 30h\n",
    "        required_time = 30\n",
    "        \n",
    "        # 2. 현재 시간을 계산\n",
    "        total_minutes = len(self.test_curr_data)\n",
    "        total_hours = np.round(total_minutes / 60,2)\n",
    "        remaining_hours = required_time - total_hours\n",
    "        remaining_minutes = int(remaining_hours * 60)  # 1055분\n",
    "        \n",
    "        # 3. 현재 시간에서 필요한 시간을 과거 데이터에서 추출\n",
    "        recent_data = self.historic_test_data.iloc[-remaining_minutes:].copy()\n",
    "        \n",
    "        # 4. 기존 데이터와 복사된 데이터를 결합\n",
    "        self.extended_df = pd.concat([recent_data, self.test_curr_data], ignore_index=True)\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\" 테스트 \n",
    "        \"\"\"\n",
    "        # 데이터 생성\n",
    "        \n",
    "        # 1. 전체 데이터 길이\n",
    "        data_len = len(self.historic_data)\n",
    "\n",
    "        # 2. 과거 데이터 길이 및 데이터\n",
    "        \n",
    "        historic_data_len = int((data_len) * 0.99)\n",
    "        self.historic_test_data = self.historic_data[:historic_data_len]\n",
    "\n",
    "        # 3. 현재 데이터 \n",
    "        self.test_curr_data = self.historic_data[historic_data_len:]\n",
    "        \n",
    "    \n",
    "    def group_3h_time_series(self):\n",
    "        \"\"\" 분 → 3h으로 리 셈플링\n",
    "        \"\"\"\n",
    "        # . 현재 시간에서 필요한 시간을 과거 데이터에서 추출\n",
    "        remaining_minutes = int(30 * 60)  # 1055분\n",
    "        recent_data = self.historic_data.iloc[-remaining_minutes:].copy()\n",
    "        \n",
    "        # 1. 데이터 길이 및 그룹 사이즈\n",
    "        data_length  = len(recent_data)\n",
    "        group_size = 180\n",
    "\n",
    "        # 2. 그룹 번호를 1부터 시작하도록 수정 (0부터 시작하는 그룹에 1을 더함)\n",
    "        groups = (np.arange(data_length) // group_size) + 1\n",
    "        \n",
    "        # 3. 그룹 변수 생성\n",
    "        recent_data['group'] = groups\n",
    "\n",
    "        # 4.그룹별로 원하는 집계 수행 (예: 평균)\n",
    "        grouped = recent_data.groupby('group').agg({'DATA_TIME': 'first', 'Moving_Average': 'mean'})\n",
    "        \n",
    "        grouped.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7f3e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesModel:\n",
    "    def __init__(self, data):\n",
    "        # 파일 경로를 지정하여 모델을 로드\n",
    "        \n",
    "        # 현재파 파일의 경로를 기준으로 model 폴더 내 모델 및 스케일러 경로 생성\n",
    "        \n",
    "        current_dir = os.getcwd()\n",
    "        time_model_relative_path = os.path.join(current_dir,\"..\" , 'src','my_package',\"model\",\"time_model_250_3h_10.pkl\")\n",
    "        scaler_model_relative_path = os.path.join(current_dir,\"..\" , 'src','my_package',\"scaler\")\n",
    "        \n",
    "        # 상대 경로를 절대 경로로 변환\n",
    "        time_model_path = os.path.abspath(time_model_relative_path)\n",
    "        scaler_model_path = os.path.abspath(scaler_model_relative_path)\n",
    "        \n",
    "        self.model = time.load_model_from_pickle(time_model_path)\n",
    "        self.scaler = time.load_model_from_pickle(scaler_model_path)\n",
    "        self.fit_into_data_format(data)\n",
    "        \n",
    "    def fit_into_data_format(self, data):\n",
    "         # 데이터 스케일링\n",
    "        scaled_data = self.scaler.fit_transform(data[['Moving_Average']])\n",
    "        \n",
    "        # 데이터 형태 조정 (LSTM input 형식)\n",
    "        self.X = scaled_data.reshape((scaled_data.shape[0], scaled_data.shape[1], 1))\n",
    "        \n",
    "        print(self.X)\n",
    "        \n",
    "    def predict(self):\n",
    "        \n",
    "        predictions = self.model.predict(self.X[0])\n",
    "        predictions = self.scaler.inverse_transform(predictions.reshape(-1, 1))\n",
    "        #y_test_rescaled = self.scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "        \n",
    "        return predictions.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed60ad61",
   "metadata": {},
   "source": [
    "함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc58ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(ship_id, pred):\n",
    "    data = get_all_dataframe_from_database('tc_ai_ecu_cleaning_cycle_model')\n",
    "    \n",
    "    # SHIP_ID가 존재하면 업데이트, 존재하지 않으면 새로운 행 추가\n",
    "    if ship_id in data['SHIP_ID'].values:\n",
    "        data.loc[data['SHIP_ID'] == ship_id, 'PRED'] = pred\n",
    "    else:\n",
    "        # 새로운 행 추가\n",
    "        new_row = {'SHIP_ID': ship_id, 'PRED': pred}\n",
    "        data = data.append(new_row, ignore_index=True)\n",
    "        \n",
    "    # 현재 날짜 가져오기\n",
    "    current_date_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # 문자열을 다시 datetime 객체로 변환\n",
    "    current_date = datetime.strptime(current_date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    data['DT_UPDATE'] = current_date\n",
    "    \n",
    "    # 중복을 피하기 위해 기존 데이터 삭제\n",
    "    delete_from_database('tc_ai_ecu_cleaning_cycle_model')\n",
    "    \n",
    "    # 수정된 데이터를 데이터베이스에 다시 저장\n",
    "    load_database(data, 'tc_ai_ecu_cleaning_cycle_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0396829",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 성공적으로 'C:\\Users\\pc021\\Desktop\\dx_project\\techross\\health_learning_data\\time_model\\src\\my_package\\model\\time_model_250_3h_10.pkl'에서 로드되었습니다.\n",
      "모델이 성공적으로 'C:\\Users\\pc021\\Desktop\\dx_project\\techross\\health_learning_data\\time_model\\src\\my_package\\scaler'에서 로드되었습니다.\n",
      "[[[1.        ]]\n",
      "\n",
      " [[0.99838397]]\n",
      "\n",
      " [[0.98970318]]\n",
      "\n",
      " [[0.99761171]]\n",
      "\n",
      " [[0.96641377]]\n",
      "\n",
      " [[0.85841872]]\n",
      "\n",
      " [[0.98222368]]\n",
      "\n",
      " [[0.26777453]]\n",
      "\n",
      " [[0.        ]]\n",
      "\n",
      " [[1.        ]]]\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002B5C3405550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 118ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc021\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.3.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\pc021\\AppData\\Local\\Temp\\ipykernel_16496\\2339041978.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append(new_row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted all records from tc_ai_ecu_cleaning_cycle_model. Deleted rows: 4\n",
      "DataFrame has been successfully loaded into tc_ai_ecu_cleaning_cycle_model table in ecs_dat1 database.\n"
     ]
    }
   ],
   "source": [
    "def caculate_prediction_value(ship_name, ship_id):\n",
    "    time_data = ConfiguringTimeData(ship_name)\n",
    "    data = time_data.group_3h_time_series()\n",
    "    \n",
    "    time_model = TimeSeriesModel(data)\n",
    "\n",
    "    predict = time_model.predict()\n",
    "    \n",
    "    create_dataset(ship_id,predict)\n",
    "    \n",
    "ship_name = 'BULK LAMBERT'\n",
    "ship_id = 'T20191002002'\n",
    "\n",
    "caculate_prediction_value(ship_name, ship_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_model",
   "language": "python",
   "name": "time_series_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
